"""
Legacy LLM Agent (Deprecated)
==============================

This is the legacy v1.0 LLM agent implementation, preserved for backward compatibility.
Use --legacy-agent flag to invoke this old behavior. Will be removed in v2.0.
"""

import json
from typing import Optional, Dict, Any
from pathlib import Path

from nova.agent.llm_client import LLMClient


class LLMAgent:
    """
    Legacy LLM agent that manages interactions with language models.
    
    This is the v1.0 implementation used in the planner-actor-critic pipeline.
    Deprecated in favor of NovaDeepAgent.
    """
    
    def __init__(self, repo_path: Path, model: str = "gpt-4"):
        """
        Initialize the legacy LLM agent.
        
        Args:
            repo_path: Path to the repository
            model: LLM model to use (e.g., "gpt-4", "gpt-3.5-turbo")
        """
        self.repo_path = repo_path
        self.model = model
        self.llm_client = LLMClient(model=model)
        self.conversation_history = []
    
    def generate_plan(self, failing_tests: str, error_details: str) -> str:
        """
        Generate a plan to fix failing tests.
        
        Args:
            failing_tests: Summary of failing tests
            error_details: Detailed error messages
            
        Returns:
            A plan as a string describing steps to fix the tests
        """
        prompt = f"""You are a software engineer tasked with fixing failing tests.

Failing Tests:
{failing_tests}

Error Details:
{error_details}

Create a step-by-step plan to fix these test failures. Be specific about:
1. What files need to be examined
2. What the likely cause of the failure is
3. What changes need to be made
4. How to verify the fix

Return only the plan, no code."""
        
        response = self.llm_client.generate(prompt, max_tokens=1000)
        self.conversation_history.append({"role": "system", "content": prompt})
        self.conversation_history.append({"role": "assistant", "content": response})
        return response
    
    def generate_patch(self, plan: str, failing_tests: str, critic_feedback: Optional[str] = None) -> str:
        """
        Generate a patch to fix the failing tests based on the plan.
        
        Args:
            plan: The plan generated by generate_plan
            failing_tests: Summary of failing tests
            critic_feedback: Optional feedback from the critic
            
        Returns:
            A patch in unified diff format
        """
        prompt = f"""Based on this plan, generate a git-format patch to fix the failing tests.

Plan:
{plan}

Failing Tests:
{failing_tests}

{f"Previous attempt feedback: {critic_feedback}" if critic_feedback else ""}

Generate a minimal patch that fixes the issues. Use unified diff format.
The patch should be applicable with 'git apply'.
Only modify source code files, not test files.

Return only the patch in diff format, starting with the diff headers."""
        
        response = self.llm_client.generate(prompt, max_tokens=2000)
        
        # Extract patch if wrapped in code blocks
        if "```diff" in response:
            start = response.find("```diff") + 7
            end = response.find("```", start)
            if end > start:
                response = response[start:end].strip()
        elif "```" in response:
            start = response.find("```") + 3
            end = response.find("```", start)
            if end > start:
                response = response[start:end].strip()
        
        return response
    
    def review_patch(self, patch: str, plan: str) -> tuple[bool, str]:
        """
        Review a patch using the critic LLM.
        
        Args:
            patch: The patch to review
            plan: The original plan
            
        Returns:
            Tuple of (approved, reason)
        """
        prompt = f"""You are a code reviewer. Review this patch for safety and correctness.

Plan:
{plan}

Patch:
{patch}

Check for:
1. Does the patch follow the plan?
2. Is the patch minimal and focused?
3. Does it avoid modifying test files?
4. Are there any obvious bugs or issues?
5. Could this break existing functionality?

Respond with a JSON object:
{{"approved": true/false, "reason": "explanation"}}"""
        
        response = self.llm_client.generate(prompt, max_tokens=500)
        
        try:
            # Try to parse JSON response
            if "{" in response and "}" in response:
                start = response.find("{")
                end = response.rfind("}") + 1
                json_str = response[start:end]
                result = json.loads(json_str)
                return result.get("approved", False), result.get("reason", "No reason provided")
        except:
            # Fallback: look for keywords
            response_lower = response.lower()
            if "approved" in response_lower or "looks good" in response_lower:
                return True, "Patch approved"
            else:
                return False, response[:200]
        
        return False, "Could not parse review response"
