"""
Critic Module for Patch Review
===============================

Reviews patches generated by the deep agent for safety and effectiveness.
"""

import re
from typing import Tuple, List, Dict, Any, Optional
from pathlib import Path


class Critic:
    """
    Critic for reviewing patches before applying them.
    
    Ensures patches are safe, minimal, and likely to fix the issues.
    """
    
    # Protected files and patterns that should not be modified
    PROTECTED_PATTERNS = [
        r"\.github/",
        r"setup\.py",
        r"pyproject\.toml",
        r"\.env",
        r"requirements.*\.txt",
        r"poetry\.lock",
        r"Pipfile",
        r"Dockerfile",
        r"\.gitignore",
        r"\.dockerignore"
    ]
    
    def __init__(
        self,
        max_lines: int = 1000,
        max_files: int = 10,
        llm_client: Optional[Any] = None
    ):
        """
        Initialize the Critic.
        
        Args:
            max_lines: Maximum allowed lines in a patch.
            max_files: Maximum allowed files to modify.
            llm_client: Optional LLM client for advanced review.
        """
        self.max_lines = max_lines
        self.max_files = max_files
        self.llm_client = llm_client
    
    def review_patch(
        self,
        patch_diff: str,
        failing_tests: List[Dict[str, Any]],
        use_llm: bool = False
    ) -> Tuple[bool, str, Dict[str, Any]]:
        """
        Review a patch for safety and effectiveness.
        
        Args:
            patch_diff: The git diff of the proposed changes.
            failing_tests: List of failing tests being addressed.
            use_llm: Whether to use LLM for advanced review.
            
        Returns:
            Tuple of (approved, feedback, metadata)
        """
        # First, perform basic safety checks
        safe, safety_reason, safety_metadata = self._safety_check(patch_diff)
        
        if not safe:
            return False, f"Safety check failed: {safety_reason}", safety_metadata
        
        # Analyze patch relevance to failing tests
        relevant, relevance_reason = self._check_relevance(
            patch_diff, 
            failing_tests
        )
        
        if not relevant:
            return False, f"Relevance check failed: {relevance_reason}", safety_metadata
        
        # If LLM review is requested and available
        if use_llm and self.llm_client:
            llm_approved, llm_feedback = self._llm_review(
                patch_diff, 
                failing_tests
            )
            if not llm_approved:
                return False, f"LLM review failed: {llm_feedback}", safety_metadata
        
        # All checks passed
        return True, "Patch approved: All checks passed", safety_metadata
    
    def _safety_check(
        self, 
        patch_diff: str
    ) -> Tuple[bool, str, Dict[str, Any]]:
        """
        Perform safety checks on the patch.
        
        Returns:
            Tuple of (is_safe, reason, metadata)
        """
        lines = patch_diff.splitlines()
        
        # Count added and removed lines
        added_lines = sum(
            1 for line in lines 
            if line.startswith('+') and not line.startswith('+++')
        )
        removed_lines = sum(
            1 for line in lines 
            if line.startswith('-') and not line.startswith('---')
        )
        
        # Extract affected files
        affected_files = []
        for line in lines:
            if line.startswith('+++') or line.startswith('---'):
                parts = line.split()
                if len(parts) >= 2:
                    path = parts[1]
                    if path.startswith('a/'):
                        path = path[2:]
                    elif path.startswith('b/'):
                        path = path[2:]
                    if path != "/dev/null" and path not in affected_files:
                        affected_files.append(path)
        
        metadata = {
            "added_lines": added_lines,
            "removed_lines": removed_lines,
            "total_lines": len(lines),
            "affected_files": affected_files,
            "file_count": len(affected_files)
        }
        
        # Check size limits
        if len(lines) > self.max_lines:
            return False, f"Patch too large ({len(lines)} lines > {self.max_lines})", metadata
        
        if len(affected_files) > self.max_files:
            return False, f"Too many files affected ({len(affected_files)} > {self.max_files})", metadata
        
        # Check for protected files
        for file_path in affected_files:
            for pattern in self.PROTECTED_PATTERNS:
                if re.search(pattern, file_path):
                    return False, f"Modifies protected file: {file_path}", metadata
        
        # Check for suspicious patterns
        suspicious_patterns = [
            r"rm\s+-rf",  # Dangerous deletion
            r"sudo\s+",   # Privilege escalation
            r"eval\(",    # Code execution
            r"exec\(",    # Code execution
            r"__import__", # Dynamic imports
            r"os\.system", # System commands
            r"subprocess\.", # System commands
        ]
        
        for line in lines:
            if line.startswith('+'):
                for pattern in suspicious_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        return False, f"Suspicious pattern detected: {pattern}", metadata
        
        return True, "Safety checks passed", metadata
    
    def _check_relevance(
        self,
        patch_diff: str,
        failing_tests: List[Dict[str, Any]]
    ) -> Tuple[bool, str]:
        """
        Check if the patch is relevant to the failing tests.
        
        Returns:
            Tuple of (is_relevant, reason)
        """
        if not failing_tests:
            return True, "No failing tests to check against"
        
        # Extract test file paths from failing tests
        test_files = set()
        for test in failing_tests:
            if "file" in test:
                test_files.add(test["file"])
        
        # Check if patch modifies test files (should not)
        lines = patch_diff.splitlines()
        for line in lines:
            if line.startswith('+++') or line.startswith('---'):
                parts = line.split()
                if len(parts) >= 2:
                    path = parts[1]
                    if path.startswith('a/'):
                        path = path[2:]
                    elif path.startswith('b/'):
                        path = path[2:]
                    
                    # Check if this is a test file
                    if path in test_files:
                        return False, f"Patch modifies test file: {path}"
                    
                    # Check for common test file patterns
                    if any(pattern in path for pattern in ["test_", "_test.py", "/tests/", "/test/"]):
                        if not path.endswith("__init__.py"):
                            return False, f"Patch modifies test file: {path}"
        
        return True, "Relevance checks passed"
    
    def _llm_review(
        self,
        patch_diff: str,
        failing_tests: List[Dict[str, Any]]
    ) -> Tuple[bool, str]:
        """
        Use LLM to review the patch.
        
        Returns:
            Tuple of (approved, feedback)
        """
        if not self.llm_client:
            return True, "LLM client not available"
        
        # Build prompt for LLM review
        prompt = f"""Review this patch for fixing failing tests.

Failing Tests:
{self._format_failing_tests(failing_tests)}

Patch:
```diff
{patch_diff}
```

Questions to consider:
1. Does this patch address the root cause of the failures?
2. Is the fix minimal and targeted?
3. Are there any potential side effects?
4. Does the code follow best practices?

Respond with APPROVED or REJECTED followed by a brief explanation."""
        
        try:
            # This would call the actual LLM client
            # response = self.llm_client.review(prompt)
            # For now, return a default approval
            return True, "LLM review not implemented"
        except Exception as e:
            return False, f"LLM review error: {e}"
    
    def _format_failing_tests(self, failing_tests: List[Dict[str, Any]]) -> str:
        """Format failing tests for display."""
        result = []
        for test in failing_tests:
            file_path = test.get("file", "unknown")
            test_name = test.get("name", "unknown")
            error = test.get("message", test.get("error", "unknown"))
            result.append(f"- {file_path}::{test_name}: {error}")
        return "\n".join(result) if result else "No failing tests"
